name: AMD GPU Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  amd-build-and-test:
    name: Build and Test on AMD GPUs
    runs-on: "amd-gfx942-mi325"
    timeout-minutes: 45
    container:
        image: rocm/pytorch:rocm6.3_ubuntu22.04_py3.10_pytorch_release_2.5.1_preview
        options: >-
          --device=/dev/kfd --device=/dev/dri --security-opt seccomp=unconfined --group-add video --user root
          --volume /home/runner/.triton:/github/home/.triton
    env:
      TRITON_BUILD_WITH_CLANG_LLD: "TRUE"
      TRITON_USE_ASSERT_ENABLED_LLVM: "TRUE"
      PYTHON: "python3"

    steps:
      - name: Install dependencies
        run: |
          apt-get update -y && apt install -y libopenmpi-dev
          pip3 install -i https://test.pypi.org/simple hip-python>=6.3.0 # (or whatever Rocm version you have)
          pip3 install pybind11
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: 'true'
      - name: Build rocmshmem bind
        run: |
          bash ./shmem/rocshmem_bind/build.sh
      - name: Build triton-distributed
        run: |
          pip3 install -e python --verbose --no-build-isolation --use-pep517
      - name: Unit tests
        run: |
          # distributed ops
          bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_distributed-notify-wait.py
          # ag gemm
          bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_ag_gemm_intra_node.py 8192 11008 4096
          # gemm rs
          bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_gemm_rs_intra_node.py 8192 4096 12288
      - name: E2E tests
        run: |
          bash ./scripts/build_e2e_env.sh
          CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_mlp.py --M 4096 --model Qwen/Qwen3-32B
          CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_mlp.py --M 4096 --model Qwen/Qwen3-32B --per_op
          # TODO: uncomment below tests until CI image has a working flash-attention installed
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_attn.py --bsz 32 --seq_len 128 --model Qwen/Qwen3-32B --mode prefill
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_attn.py --bsz 4096 --seq_len 128 --model Qwen/Qwen3-32B --mode decode
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_e2e.py --bsz 16 --seq_len 256 --model Qwen/Qwen3-32B --check
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_e2e.py --bsz 32 --seq_len 128 --model Qwen/Qwen3-32B --mode prefill
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_tp_e2e.py --bsz 4096 --seq_len 128 --model Qwen/Qwen3-32B --mode decode
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_e2e_inference.py --bsz 4096 --gen_len 128 --max_length 150
          # CUDA_GRAPH=1 bash ./scripts/launch_amd.sh ./python/triton_dist/test/amd/test_e2e_inference.py --bsz 4096 --gen_len 128 --max_length 150 --triton_dist
